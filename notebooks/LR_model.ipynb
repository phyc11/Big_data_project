{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "322ec755",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92304901-50e6-4ec5-b57b-e6bdbc39a9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read HDFS Weather Data\") \\\n",
    "    .master(\"local\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:9000\") \\\n",
    "    .getOrCreate()\n",
    "# spark://spark-master:7077"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37483270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(location):\n",
    "    df = spark.read.option(\"multiLine\", True) \\\n",
    "        .option(\"header\", True) \\\n",
    "        .option(\"inferSchema\", False) \\\n",
    "        .option(\"encoding\", \"utf-8\") \\\n",
    "        .csv(f\"hdfs://namenode:9000/tmp/weather_data/history/{location}.csv\")\n",
    "\n",
    "    df = df.toPandas()\n",
    "    \n",
    "    # Xóa các số đo, chỉ lấy giá trị\n",
    "    df['Temp'] = df['Temp'].str.replace('°c', '').str.strip()\n",
    "    df['Rain'] = df['Rain'].str.replace('\\nmm', '').str.strip()\n",
    "    df['Cloud'] = df['Cloud'].str.replace('%', '').str.strip()\n",
    "    df['Pressure'] = df['Pressure'].str.replace('mb', '').str.strip()\n",
    "    df['Wind'] = df['Wind'].str.replace('km/h', '').str.strip()\n",
    "    df['Gust'] = df['Gust'].str.replace('km/h', '').str.strip()\n",
    "\n",
    "    df = df.astype({\n",
    "        'Date': 'datetime64[ns]',\n",
    "        'Temp': 'float64',\n",
    "        'Rain': 'float64',\n",
    "        'Cloud': 'float64',\n",
    "        'Pressure': 'float64',\n",
    "        'Wind': 'float64',\n",
    "        'Gust': 'float64'\n",
    "    })\n",
    "\n",
    "    df = df.rename(columns={\n",
    "        'Temp': 'Temp(°c)',\n",
    "        'Rain': 'Rain(nmm)',\n",
    "        'Cloud': 'Cloud(%)',\n",
    "        'Pressure': 'Pressure(mb)',\n",
    "        'Wind': 'Wind(km/h)',\n",
    "        'Gust': 'Gust(km/h)'\n",
    "    })\n",
    "\n",
    "    #Nhóm dữ liệu\n",
    "    weather_type1 = ['Sunny', 'Clear', 'Partly cloudy']\n",
    "    weather_type2 = ['Overcast', 'Cloudy', 'Patchy rain possible', 'Light drizzle', 'Light rain shower', 'Patchy light rain with thunder']\n",
    "    weather_type3 = ['Heavy rain at times', 'Moderate or heavy rain shower', 'Moderate rain at times', 'Moderate rain']\n",
    "\n",
    "    # Áp dụng số hóa cho cột 'Weather'\n",
    "    conditions = [\n",
    "        df['Weather'].isin(weather_type1),\n",
    "        df['Weather'].isin(weather_type2),\n",
    "        df['Weather'].isin(weather_type3)\n",
    "    ]\n",
    "    choices = [0, 1, 2]\n",
    "\n",
    "    df['Weather'] = np.select(conditions, choices, default=0)\n",
    "\n",
    "    #Lấy thông tin 6 mốc gần nhất để thêm dữ kiện\n",
    "    lag_steps = 3\n",
    "    for lag in range(1, lag_steps + 1):\n",
    "        df[f'Temp_t-{lag}'] = df['Temp(°c)'].shift(lag)\n",
    "        df[f'Rain_t-{lag}'] = df['Rain(nmm)'].shift(lag)\n",
    "        df[f'Cloud_t-{lag}'] = df['Cloud(%)'].shift(lag)\n",
    "        df[f'Pressure_t-{lag}'] = df['Pressure(mb)'].shift(lag)\n",
    "        df[f'Wind_t-{lag}'] = df['Wind(km/h)'].shift(lag)\n",
    "        df[f'Gust_t-{lag}'] = df['Gust(km/h)'].shift(lag)\n",
    "\n",
    "    df.drop(columns=['Date', 'Time'], inplace=True)\n",
    "\n",
    "    # Bỏ qua các hàng có giá trị NaN\n",
    "    X = df.drop(columns=['Weather'])\n",
    "    Y = df['Weather']\n",
    "    X = X[lag_steps:-1].reset_index(drop=True)\n",
    "    Y = Y[lag_steps + 1:].reset_index(drop=True)\n",
    "    X['Weather'] = Y\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81e40e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xây dựng pipeline và mô hình Logistic Regression\n",
    "def build_lr_model(location):\n",
    "    df_pd = pd.DataFrame()\n",
    "    # for location in provinces:\n",
    "    #     df_cur = data_preprocess(location)\n",
    "    #     df_pd = pd.concat([df_pd, df_cur], ignore_index= True)\n",
    "    df_cur= data_preprocess(location)\n",
    "    df_pd = pd.concat([df_pd, df_cur], ignore_index= True)\n",
    "    # Chuyển Pandas sang DataFrame Spark\n",
    "    df_loc = spark.createDataFrame(df_pd)\n",
    "\n",
    "    # Danh sách feature\n",
    "    feature_cols = [c for c in df_pd.columns if c != 'Weather']\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_assembled\")\n",
    "    scaler = StandardScaler(inputCol=\"features_assembled\", outputCol=\"features\")\n",
    "    lr = LogisticRegression(labelCol=\"Weather\", featuresCol=\"features\", maxIter=100)\n",
    "\n",
    "    pipeline = Pipeline(stages=[assembler, scaler, lr])\n",
    "\n",
    "    # Grid Search tham số\n",
    "    paramGrid = (ParamGridBuilder()\n",
    "        .addGrid(lr.regParam, [0.1])\n",
    "        .addGrid(lr.elasticNetParam, [0.5])\n",
    "        .build())\n",
    "\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"Weather\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "    cv = CrossValidator(estimator=pipeline,\n",
    "                        estimatorParamMaps=paramGrid,\n",
    "                        evaluator=evaluator,\n",
    "                        numFolds=3)\n",
    "\n",
    "    # Chia train/test theo tỉ lệ 80/20\n",
    "    train_df, test_df = df_loc.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "    # Huấn luyện với CrossValidator\n",
    "    cv_model = cv.fit(train_df)\n",
    "\n",
    "    # Đánh giá\n",
    "    predictions = cv_model.transform(test_df)\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    print(f\"Logistic Regression Accuracy = {accuracy}\")\n",
    "\n",
    "    return cv_model, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e11259f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "northern_provinces = [\n",
    "    'bac-can', 'bac-giang', 'bac-ninh', \n",
    "    'dien-bien', 'ha-giang', 'ha-noi', 'hai-duong', \n",
    "    'hai-phong', 'hoa-binh', 'hong-gai', \n",
    "    'lang-son', 'lao-cai', 'nam-dinh', 'ninh-binh', \n",
    "    'phu-ly', 'son-la', 'son-tay', 'thai-binh', \n",
    "    'thai-nguyen', 'tuyen-quang', 'uong-bi', 'viet-tri', \n",
    "    'vinh-yen'\n",
    "]\n",
    "\n",
    "central_provinces = [\n",
    "    'da-lat', 'dong-hoi', 'ha-tinh', 'hoi-an', \n",
    "    'hue', 'kon-tum', 'nha-trang', 'phan-rang', \n",
    "    'phan-thiet', 'play-cu', 'quang-ngai', 'qui-nhon', \n",
    "    'tam-ky', 'thanh-hoa', 'tuy-hoa', 'vinh', \n",
    "    'buon-me-thuot', 'cam-ranh'\n",
    "]\n",
    "\n",
    "southern_provinces = [\n",
    "    'bac-lieu', 'ben-tre', 'bien-hoa', 'ca-mau', \n",
    "    'chau-doc', 'dong-xoai', 'ho-chi-minh-city', \n",
    "    'long-xuyen', 'my-tho', 'rach-gia', 'soc-trang', \n",
    "    'tan-an', 'tay-ninh', 'tra-vinh', 'vinh-long', \n",
    "    'vung-tau'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1654ccfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy = 0.76811918897726\n",
      "Finished training for central provinces. Accuracy: 0.76811918897726\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lr_model, lr_acc = build_lr_model('vinh')\n",
    "print(\"Finished training for central provinces. Accuracy:\", lr_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27c97703-f76a-4e3a-891c-222e0eb71086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lưu model\n",
    "#model_path = \"./built_model/lr_central_model\"\n",
    "#lr_model.write().overwrite().save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81d019e4-b8f5-44f2-97ed-056b9141e837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.ml.tuning.CrossValidatorModel'>\n"
     ]
    }
   ],
   "source": [
    "print(type(lr_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6739dae2-bf2d-4c38-a501-67b9cd151a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lại model\n",
    "#from pyspark.ml.tuning import CrossValidatorModel\n",
    "#loaded_model = CrossValidatorModel.load(\"./built_model/lr_central_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a85d628-4971-4645-b8f9-4b56afa480c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
