{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b13647b3-0e5d-42f2-aad2-5b43fc1e60ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler  \n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fecf3969-7418-4ee8-b78c-9bdecc162dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read HDFS Weather Data\") \\\n",
    "    .master(\"local\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:9000\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51760576-1686-46e9-ab9f-266db42b4d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def data_preprocess(location, target_variable='all'):\n",
    "   \n",
    "    # Read data from HDFS\n",
    "    df = spark.read.option(\"multiLine\", True) \\\n",
    "        .option(\"header\", True) \\\n",
    "        .option(\"inferSchema\", False) \\\n",
    "        .option(\"encoding\", \"utf-8\") \\\n",
    "        .csv(f\"hdfs://namenode:9000/tmp/weather_data/history/{location}.csv\")\n",
    "    df = df.toPandas()\n",
    "    \n",
    "    df['Temp'] = df['Temp'].str.replace('Â°c', '').str.strip()\n",
    "    df['Rain'] = df['Rain'].str.replace('\\nmm', '').str.strip()\n",
    "    df['Cloud'] = df['Cloud'].str.replace('%', '').str.strip()\n",
    "    df['Pressure'] = df['Pressure'].str.replace('mb', '').str.strip()\n",
    "    df['Wind'] = df['Wind'].str.replace('km/h', '').str.strip()\n",
    "    df['Gust'] = df['Gust'].str.replace('km/h', '').str.strip()\n",
    "    \n",
    "    df = df.astype({\n",
    "        'Date': 'datetime64[ns]',\n",
    "        'Temp': 'float64',\n",
    "        'Rain': 'float64',\n",
    "        'Cloud': 'float64',\n",
    "        'Pressure': 'float64',\n",
    "        'Wind': 'float64',\n",
    "        'Gust': 'float64'\n",
    "    })\n",
    "    \n",
    "    df = df.rename(columns={\n",
    "        'Temp': 'temp',\n",
    "        'Rain': 'rain',\n",
    "        'Cloud': 'cloud',\n",
    "        'Pressure': 'pressure',\n",
    "        'Wind': 'windspeed',\n",
    "        'Gust': 'Gust'\n",
    "    })\n",
    "    \n",
    "    weather_type1 = ['Sunny', 'Clear', 'Partly cloudy']\n",
    "    weather_type2 = ['Overcast', 'Cloudy', 'Patchy rain possible', 'Light drizzle', 'Light rain shower', 'Patchy light rain with thunder']\n",
    "    weather_type3 = ['Heavy rain at times', 'Moderate or heavy rain shower', 'Moderate rain at times', 'Moderate rain']\n",
    "    \n",
    "    conditions = [\n",
    "        df['Weather'].isin(weather_type1),\n",
    "        df['Weather'].isin(weather_type2),\n",
    "        df['Weather'].isin(weather_type3)\n",
    "    ]\n",
    "    choices = [0, 1, 2]\n",
    "    df['Weather'] = np.select(conditions, choices, default=0)\n",
    "    \n",
    "    \n",
    "    lag_steps = 3\n",
    "    for lag in range(1, lag_steps + 1):\n",
    "        df[f'temp_lag_{lag}'] = df['temp'].shift(lag)\n",
    "        df[f'rain_lag_{lag}'] = df['rain'].shift(lag)\n",
    "        df[f'cloud_lag_{lag}'] = df['cloud'].shift(lag)\n",
    "        df[f'pressure_lag_{lag}'] = df['pressure'].shift(lag)\n",
    "        df[f'windspeed_lag_{lag}'] = df['windspeed'].shift(lag)\n",
    "        df[f'Gust_lag_{lag}'] = df['Gust'].shift(lag)\n",
    "    \n",
    "    # Save original values for future targets\n",
    "    future_temp = df['temp'].iloc[lag_steps + 1:].reset_index(drop=True)\n",
    "    future_cloud = df['cloud'].iloc[lag_steps + 1:].reset_index(drop=True)\n",
    "    future_weather = df['Weather'].iloc[lag_steps + 1:].reset_index(drop=True)\n",
    "    \n",
    "    # Drop date/time columns\n",
    "    df.drop(columns=['Date', 'Time'], inplace=True)\n",
    "    \n",
    "    # Create feature set (all columns except target variables)\n",
    "    X = df.iloc[lag_steps:-1].reset_index(drop=True)\n",
    "    \n",
    "    result = {}\n",
    "    \n",
    "    # Prepare datasets for each target variable\n",
    "    if target_variable in ['all', 'weather']:\n",
    "        X_weather = X.copy()\n",
    "        X_weather['Weather'] = future_weather\n",
    "        result['weather'] = X_weather\n",
    "    \n",
    "    if target_variable in ['all', 'temperature']:\n",
    "        X_temp = X.copy()\n",
    "        X_temp['Future_Temp'] = future_temp\n",
    "        result['temperature'] = X_temp\n",
    "    \n",
    "    if target_variable in ['all', 'cloud']:\n",
    "        X_cloud = X.copy()\n",
    "        X_cloud['Future_Cloud'] = future_cloud\n",
    "        result['cloud'] = X_cloud\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4573179-c6d5-4082-bdf5-8e3dea89d608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_weather_prediction_model(provinces):\n",
    "    \"\"\"\n",
    "    Build models to predict weather conditions\n",
    "    \"\"\"\n",
    "    df_pd = pd.DataFrame()\n",
    "    for location in provinces:\n",
    "        data_dict = data_preprocess(location, 'weather')\n",
    "        df_cur = data_dict['weather']\n",
    "        df_pd = pd.concat([df_pd, df_cur], ignore_index=True)\n",
    "    \n",
    "    df = spark.createDataFrame(df_pd)\n",
    "    \n",
    "    feature_cols = [col for col in df_pd.columns if col != 'Weather']\n",
    "    \n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_assembled\")\n",
    "    scaler = StandardScaler(inputCol=\"features_assembled\", outputCol=\"features\")\n",
    "    \n",
    "    # Random Forest classifier for weather condition prediction\n",
    "    rf = RandomForestClassifier(labelCol=\"Weather\", featuresCol=\"features\")\n",
    "    \n",
    "    # Define pipeline\n",
    "    pipeline = Pipeline(stages=[assembler, scaler, rf])\n",
    "    \n",
    "    # Grid search for hyperparameter tuning\n",
    "    paramGrid = (ParamGridBuilder()\n",
    "        .addGrid(rf.numTrees, [50])\n",
    "        .addGrid(rf.maxDepth, [5])\n",
    "        .addGrid(rf.minInstancesPerNode, [2])\n",
    "        .build())\n",
    "    \n",
    "    # Evaluation & cross validation\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"Weather\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    cv = CrossValidator(estimator=pipeline,\n",
    "                        estimatorParamMaps=paramGrid,\n",
    "                        evaluator=evaluator,\n",
    "                        numFolds=5)\n",
    "    \n",
    "    # Split train/test\n",
    "    train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
    "    \n",
    "    # Fit model\n",
    "    cv_model = cv.fit(train_data)\n",
    "    \n",
    "    # Evaluate model\n",
    "    predictions = cv_model.transform(test_data)\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    print(f\"Weather Prediction Accuracy = {accuracy}\")\n",
    "    \n",
    "    return cv_model, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f96cdcda-c024-47fd-bccb-0c7ce83012b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_temperature_prediction_model(provinces):\n",
    "    \"\"\"\n",
    "    Build model to predict future temperature\n",
    "    \"\"\"\n",
    "    df_pd = pd.DataFrame()\n",
    "    for location in provinces:\n",
    "        data_dict = data_preprocess(location, 'temperature')\n",
    "        df_cur = data_dict['temperature']\n",
    "        df_pd = pd.concat([df_pd, df_cur], ignore_index=True)\n",
    "    \n",
    "    # Convert pandas DataFrame to Spark DataFrame\n",
    "    df = spark.createDataFrame(df_pd)\n",
    "    \n",
    "    # Define feature columns (all except 'Future_Temp')\n",
    "    feature_cols = [col for col in df_pd.columns if col != 'Future_Temp']\n",
    "    \n",
    "    # Create feature vector\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_assembled\")\n",
    "    scaler = StandardScaler(inputCol=\"features_assembled\", outputCol=\"features\")\n",
    "    \n",
    "    # Random Forest regressor for temperature prediction\n",
    "    rf = RandomForestRegressor(labelCol=\"Future_Temp\", featuresCol=\"features\")\n",
    "    \n",
    "    # Define pipeline\n",
    "    pipeline = Pipeline(stages=[assembler, scaler, rf])\n",
    "    \n",
    "    # Grid search for hyperparameter tuning\n",
    "    paramGrid = (ParamGridBuilder()\n",
    "        .addGrid(rf.numTrees, [50])\n",
    "        .addGrid(rf.maxDepth, [5])\n",
    "        .addGrid(rf.minInstancesPerNode, [2])\n",
    "        .build())\n",
    "    \n",
    "    # Evaluation & cross validation\n",
    "    evaluator = RegressionEvaluator(labelCol=\"Future_Temp\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    cv = CrossValidator(estimator=pipeline,\n",
    "                        estimatorParamMaps=paramGrid,\n",
    "                        evaluator=evaluator,\n",
    "                        numFolds=5)\n",
    "    \n",
    "    # Split train/test\n",
    "    train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
    "    \n",
    "    # Fit model\n",
    "    cv_model = cv.fit(train_data)\n",
    "    \n",
    "    # Evaluate model\n",
    "    predictions = cv_model.transform(test_data)\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    print(f\"Temperature Prediction RMSE = {rmse}\")\n",
    "    \n",
    "    return cv_model, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16004be3-8b23-48da-b511-dbe44209583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cloud_prediction_model(provinces):\n",
    "    \"\"\"\n",
    "    Build model to predict future cloud cover\n",
    "    \"\"\"\n",
    "    df_pd = pd.DataFrame()\n",
    "    for location in provinces:\n",
    "        data_dict = data_preprocess(location, 'cloud')\n",
    "        df_cur = data_dict['cloud']\n",
    "        df_pd = pd.concat([df_pd, df_cur], ignore_index=True)\n",
    "    \n",
    "    # Convert pandas DataFrame to Spark DataFrame\n",
    "    df = spark.createDataFrame(df_pd)\n",
    "    \n",
    "    # Define feature columns (all except 'Future_Cloud')\n",
    "    feature_cols = [col for col in df_pd.columns if col != 'Future_Cloud']\n",
    "    \n",
    "    # Create feature vector\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_assembled\")\n",
    "    scaler = StandardScaler(inputCol=\"features_assembled\", outputCol=\"features\")\n",
    "    \n",
    "    # Random Forest regressor for cloud cover prediction\n",
    "    rf = RandomForestRegressor(labelCol=\"Future_Cloud\", featuresCol=\"features\")\n",
    "    \n",
    "    # Define pipeline\n",
    "    pipeline = Pipeline(stages=[assembler, scaler, rf])\n",
    "    \n",
    "    # Grid search for hyperparameter tuning\n",
    "    paramGrid = (ParamGridBuilder()\n",
    "        .addGrid(rf.numTrees, [50])\n",
    "        .addGrid(rf.maxDepth, [5])\n",
    "        .addGrid(rf.minInstancesPerNode, [2])\n",
    "        .build())\n",
    "    \n",
    "    # Evaluation & cross validation\n",
    "    evaluator = RegressionEvaluator(labelCol=\"Future_Cloud\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    cv = CrossValidator(estimator=pipeline,\n",
    "                        estimatorParamMaps=paramGrid,\n",
    "                        evaluator=evaluator,\n",
    "                        numFolds=5)\n",
    "    \n",
    "    # Split train/test\n",
    "    train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
    "    \n",
    "    # Fit model\n",
    "    cv_model = cv.fit(train_data)\n",
    "    \n",
    "    # Evaluate model\n",
    "    predictions = cv_model.transform(test_data)\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    print(f\"Cloud Cover Prediction RMSE = {rmse}\")\n",
    "    \n",
    "    return cv_model, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "699a2ca9-1280-4e31-99b8-272acf534e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather Prediction Accuracy = 0.7251527939464494\n",
      "Temperature Prediction RMSE = 1.9277850996300645\n",
      "Cloud Cover Prediction RMSE = 19.25699001619655\n",
      "Model Performance Summary:\n",
      "Weather Condition Prediction Accuracy: 0.7252\n",
      "Temperature Prediction RMSE: 1.9278Â°C\n",
      "Cloud Cover Prediction RMSE: 19.2570%\n"
     ]
    }
   ],
   "source": [
    "def build_combined_weather_models(provinces):\n",
    "    \n",
    "    weather_model, weather_accuracy = build_weather_prediction_model(provinces)\n",
    "    temp_model, temp_rmse = build_temperature_prediction_model(provinces)\n",
    "    cloud_model, cloud_rmse = build_cloud_prediction_model(provinces)\n",
    "\n",
    "    print(\"Model Performance Summary:\")\n",
    "    print(f\"Weather Condition Prediction Accuracy: {weather_accuracy:.4f}\")  \n",
    "    print(f\"Temperature Prediction RMSE: {temp_rmse:.4f}Â°C\")\n",
    "    print(f\"Cloud Cover Prediction RMSE: {cloud_rmse:.4f}%\")\n",
    "    \n",
    "    return {\n",
    "        \"weather_model\": weather_model,\n",
    "        \"temperature_model\": temp_model,\n",
    "        \"cloud_model\": cloud_model\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    provinces = [\"ha-noi\", \"ho-chi-minh-city\", \"vinh\"]  # Add your actual province names\n",
    "    models = build_combined_weather_models(provinces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc5e2478-7863-480a-a383-f9d7b35ce8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÄÃ£ lÆ°u weather_model\n",
      "ÄÃ£ lÆ°u temperature_model\n",
      "ÄÃ£ lÆ°u cloud_model\n"
     ]
    }
   ],
   "source": [
    "model_path_org = \"hdfs://namenode:9000/model/rf_model\"\n",
    "\n",
    "for name,model in models.items():\n",
    "    model_path = f\"{model_path_org}/{name}\"\n",
    "    best_model=model.bestModel\n",
    "    best_model.save(model_path)\n",
    "    print(f\"ÄÃ£ lÆ°u {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aae22220-e053-4325-a3f5-9a1ad7f6de88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÄÃ£ lÆ°u weather_model\n",
      "ÄÃ£ lÆ°u temperature_model\n",
      "ÄÃ£ lÆ°u cloud_model\n"
     ]
    }
   ],
   "source": [
    "model_path_local = \"model\"\n",
    "\n",
    "for name,model in models.items():\n",
    "    model_path = f\"{model_path_local}/{name}\"\n",
    "    best_model=model.bestModel\n",
    "    best_model.save(model_path)\n",
    "    print(f\"ÄÃ£ lÆ°u {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4b9c466-25d4-483a-a4b9-03c1c540138d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÄÃ£ lÆ°u weather_model vÃ o file:///home/jovyan/notebooks/model/weather_model\n",
      "ÄÃ£ lÆ°u temperature_model vÃ o file:///home/jovyan/notebooks/model/temperature_model\n",
      "ÄÃ£ lÆ°u cloud_model vÃ o file:///home/jovyan/notebooks/model/cloud_model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model_path_local = \"/home/jovyan/notebooks/model\"  # hoáº·c ÄÆ°á»ng dáº«n tuyá»t Äá»i khÃ¡c trong container\n",
    "os.makedirs(model_path_local, exist_ok=True)\n",
    "\n",
    "for name, model in models.items():\n",
    "    model_path = f\"file://{model_path_local}/{name}\"  # LÆ°u Ã½: file://\n",
    "    best_model = model.bestModel\n",
    "    best_model.save(model_path)\n",
    "    print(f\"ÄÃ£ lÆ°u {name} vÃ o {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235decaa-6d61-45da-a06f-11e49a21cb0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
